{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer like LLM Implementation for Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7X5j3GqeuX8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMASH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\AMASH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import torch.optim as optim\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer\n",
    "from tensorflow.keras.layers import Layer, Dense, Dropout, LayerNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2O8wvibYqf-f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMASH\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AMASH\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Datasets Selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7hkxawF5bLwn",
    "outputId": "2a969133-0f2c-4156-8f63-4c44a43a2de5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            dialogue  \\\n",
      "0  Amanda: I baked  cookies. Do you want some?\\r\\...   \n",
      "1  Olivia: Who are you voting for in this electio...   \n",
      "2  Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I wa...   \n",
      "3  Edward: Rachel, I think I'm in ove with Bella....   \n",
      "4  Sam: hey  overheard rick say something\\r\\nSam:...   \n",
      "\n",
      "                                             summary  \n",
      "0  Amanda baked cookies and will bring Jerry some...  \n",
      "1  Olivia and Olivier are voting for liberals in ...  \n",
      "2  Kim may try the pomodoro technique recommended...  \n",
      "3  Edward thinks he is in love with Bella. Rachel...  \n",
      "4  Sam is confused, because he overheard Rick com...  \n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('Datasets/samsum-train.csv')\n",
    "test = pd.read_csv('Datasets/samsum-test.csv')\n",
    "val = pd.read_csv('Datasets/samsum-validation.csv')\n",
    "\n",
    "train = train.dropna()\n",
    "test = test.dropna()\n",
    "val = val.dropna()\n",
    "\n",
    "print(train[['dialogue', 'summary']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aisUWCLXfYXw",
    "outputId": "98ca8df5-3d40-4be2-8dbd-714d22216ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11348    Nelly: Beer after work?\\r\\nNina: Can't, not to...\n",
      "10583    James: hiya do you know whats wring with our w...\n",
      "2272     Ben: Tomorrow is the submission deadline.\\r\\nJ...\n",
      "14689    Alex: Did you hear the newest song from Anne M...\n",
      "25       Julius: dude, your assessment of manutd\\r\\nLaw...\n",
      "Name: dialogue, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\AMASH\\AppData\\Local\\Temp\\ipykernel_26572\\3975873107.py:5: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  clean = '\\n'.join([line for line in clean.split('\\n') if not re.match('.*:\\s*$', line)])\n"
     ]
    }
   ],
   "source": [
    "def clean_tags(text):\n",
    "    clean = re.compile('<.*?>') \n",
    "    clean = re.sub(clean, '', text) \n",
    "    \n",
    "    clean = '\\n'.join([line for line in clean.split('\\n') if not re.match('.*:\\s*$', line)])\n",
    "\n",
    "    return clean\n",
    "\n",
    "def clean_df(df, cols):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].fillna('').apply(clean_tags)\n",
    "    return df\n",
    "\n",
    "train = clean_df(train,['dialogue', 'summary'])\n",
    "test = clean_df(test,['dialogue', 'summary'])\n",
    "val = clean_df(val,['dialogue', 'summary'])\n",
    "\n",
    "print(train['dialogue'].sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Format Selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bx5x-FpLfqbu",
    "outputId": "051f994a-7ac3-428c-b18e-d6ed2a1db5cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'dialogue', 'summary', '__index_level_0__'],\n",
      "    num_rows: 14731\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "Dataset({\n",
      "    features: ['id', 'dialogue', 'summary'],\n",
      "    num_rows: 819\n",
      "})\n",
      "\n",
      "\n",
      "\n",
      "Dataset({\n",
      "    features: ['id', 'dialogue', 'summary'],\n",
      "    num_rows: 818\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_ds = Dataset.from_pandas(train)\n",
    "test_ds = Dataset.from_pandas(test)\n",
    "val_ds = Dataset.from_pandas(val)\n",
    "\n",
    "train_ds.set_format(type='tensorflow', columns=['dialogue', 'summary'])\n",
    "test_ds.set_format(type='tensorflow', columns=['dialogue', 'summary'])\n",
    "val_ds.set_format(type='tensorflow', columns=['dialogue', 'summary'])\n",
    "\n",
    "print(train_ds)\n",
    "print('\\n' * 2)\n",
    "print(test_ds)\n",
    "print('\\n' * 2)\n",
    "print(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JbmoY6F__83i",
    "outputId": "5afa8e03-d2ab-4168-e7e4-1bffa59d21df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dialogue', 'summary']\n",
      "['id', 'dialogue', 'summary']\n",
      "['id', 'dialogue', 'summary']\n"
     ]
    }
   ],
   "source": [
    "#columns removing\n",
    "train_ds = train_ds.remove_columns(['__index_level_0__'])\n",
    "train_ds = train_ds.remove_columns(['id'])\n",
    "\n",
    "print(train_ds.column_names)\n",
    "print(test_ds.column_names)\n",
    "print(val_ds.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Selecting Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "w66m2gMHItv0"
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "num_layers = 8\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "dff = 2048\n",
    "max_len = 256\n",
    "dropout_rate = 0.1\n",
    "EPOCHS = 2\n",
    "vocab_size = tokenizer.vocab_size\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creation of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "VgKO_NXFIVd6"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_create_dataset_hf(dataset, batch_size, shuffle=True):\n",
    "    input_texts = [str(tensor.numpy().decode('utf-8')) for tensor in dataset['dialogue']]\n",
    "    target_texts = [str(tensor.numpy().decode('utf-8')) for tensor in dataset['summary']]\n",
    "\n",
    "    #tokenize inputs\n",
    "    input_features = tokenizer(\n",
    "        input_texts,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "    input_ids = input_features[\"input_ids\"]\n",
    "    attention_mask = input_features[\"attention_mask\"]\n",
    "\n",
    "    #tokenize targets\n",
    "    target_ids = tokenizer(\n",
    "        target_texts,\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"tf\"\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    #tensorflow data\n",
    "    tf_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }, target_ids))\n",
    "\n",
    "    if shuffle:\n",
    "        tf_dataset = tf_dataset.shuffle(buffer_size=len(input_ids))\n",
    "    tf_dataset = tf_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E_kXpMJtGwld",
    "outputId": "e6788a82-d1cb-46eb-9c44-ea94acf3154b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 256), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 256), dtype=tf.int32, name=None)}, TensorSpec(shape=(None, 256), dtype=tf.int32, name=None))>\n",
      "<_PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 256), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 256), dtype=tf.int32, name=None)}, TensorSpec(shape=(None, 256), dtype=tf.int32, name=None))>\n",
      "<_PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(None, 256), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 256), dtype=tf.int32, name=None)}, TensorSpec(shape=(None, 256), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tokenize_and_create_dataset_hf(train_ds, batch_size)\n",
    "test_dataset = tokenize_and_create_dataset_hf(test_ds, batch_size, shuffle=False)\n",
    "val_dataset = tokenize_and_create_dataset_hf(val_ds, batch_size, shuffle=False)\n",
    "\n",
    "print(train_dataset)\n",
    "print(test_dataset)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gUnvAsJl4nze"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(Layer):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.positional_encoding = self.compute_positional_encoding(d_model, max_len)\n",
    "\n",
    "    def compute_positional_encoding(self, d_model, max_len):\n",
    "        position = np.arange(max_len)[:, np.newaxis]\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "        pe = np.zeros((max_len, d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "        return tf.cast(pe[np.newaxis, ...], dtype=tf.float32)  # Shape: (1, max_len, d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]  # Get the sequence length\n",
    "        return x + self.positional_encoding[:, :seq_len, :]  # Add positional encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "N_dAvrHp5H3l"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "        self.dense = Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Split into heads and transpose\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])  # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "    def call(self, v, k, q, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        # Linear projections\n",
    "        q = self.split_heads(self.wq(q), batch_size)\n",
    "        k = self.split_heads(self.wk(k), batch_size)\n",
    "        v = self.split_heads(self.wv(v), batch_size)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "        attention_output = tf.matmul(attention_weights, v)  # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        # Concatenate heads and project back to d_model\n",
    "        attention_output = tf.transpose(attention_output, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, depth)\n",
    "        concat_attention = tf.reshape(attention_output, (batch_size, -1, self.num_heads * self.depth))  # (batch_size, seq_len, d_model)\n",
    "        output = self.dense(concat_attention)  # Final output shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qwmSsgWx5WYQ"
   },
   "outputs": [],
   "source": [
    "# Feed-Forward Network\n",
    "class PointWiseFeedForwardNetwork(Layer):\n",
    "    def __init__(self, d_model, dff):\n",
    "        super(PointWiseFeedForwardNetwork, self).__init__()\n",
    "        self.dense1 = Dense(dff, activation='relu')\n",
    "        self.dense2 = Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        return self.dense2(self.dense1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "m3DQttir5vCh"
   },
   "outputs": [],
   "source": [
    "class Decoder(Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, max_len, dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)  # Embedding layer\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.final_layer = Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, training=False, look_ahead_mask=None):\n",
    "        seq_len = tf.shape(x)[1]  # Get the sequence length of the input\n",
    "        x = self.embedding(x) * tf.math.sqrt(tf.cast(self.d_model, tf.float32))  # Embedding with scaling\n",
    "        x = self.pos_encoding(x)  # Add positional encoding\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, training=training, look_ahead_mask=look_ahead_mask)\n",
    "\n",
    "        return self.final_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lTpO0qylh-qv"
   },
   "outputs": [],
   "source": [
    "class DecoderTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, max_len, dropout_rate=0.1):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, vocab_size, max_len, dropout_rate)\n",
    "\n",
    "    def call(self, tar, training=None, look_ahead_mask=None):\n",
    "        return self.decoder(tar, training=training, look_ahead_mask=look_ahead_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fRiHhLdzuJYY"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PointWiseFeedForwardNetwork(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = Dropout(dropout_rate)\n",
    "        self.dropout2 = Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x, training, look_ahead_mask):\n",
    "        # Multi-head self-attention\n",
    "        attn_output = self.mha(x, x, x, look_ahead_mask)  # (batch_size, seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # Residual connection + LayerNorm\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # Residual connection + LayerNorm\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summarizing the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "Dialogue:\n",
      "Hannah: Hey, do you have Betty's number?\n",
      "Amanda: Lemme check\n",
      "Amanda: Sorry, can't find it.\n",
      "Amanda: Ask Larry\n",
      "Amanda: He called her last time we were at the park together\n",
      "Hannah: I don't know him well\n",
      "Amanda: Don't be shy, he's very nice\n",
      "Hannah: If you say so..\n",
      "Hannah: I'd rather you texted him\n",
      "Amanda: Just text him ðŸ™‚\n",
      "Hannah: Urgh.. Alright\n",
      "Hannah: Bye\n",
      "Amanda: Bye bye\n",
      "\n",
      "Generated Summary:\n",
      "Hannah: Hey, do you have Betty's number? Amanda: Lemme check. Ask Larry. He called her last time we were at the park together. He's very nice, he's a good guy. Amanda texts him: Urgh.. Alright.\n",
      "\n",
      "Example 2:\n",
      "Dialogue:\n",
      "Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially like the train part!\n",
      "Rob: Hahaha! No one talks to the machine like that!\n",
      "Eric: Is this his only stand-up?\n",
      "Rob: Idk. I'll check.\n",
      "Eric: Sure.\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
      "Eric: Gr8! I'll watch them now!\n",
      "Rob: Me too!\n",
      "Eric: MACHINE!\n",
      "Rob: MACHINE!\n",
      "Eric: TTYL?\n",
      "Rob: Sure :)\n",
      "\n",
      "Generated Summary:\n",
      "Rob: Is this his only stand-up? Eric: Sure. Rob: Idk. There are some of his stand-ups on youtube. Eric: Gr8! I'll watch them now! Rob: Hahaha! No one talks to the machine like that!\n",
      "\n",
      "Example 3:\n",
      "Dialogue:\n",
      "Lenny: Babe, can you help me with something?\n",
      "Bob: Sure, what's up?\n",
      "Lenny: Which one should I pick?\n",
      "Bob: Send me photos\n",
      "Bob: I like the first ones best\n",
      "Lenny: But I already have purple trousers. Does it make sense to have two pairs?\n",
      "Bob: I have four black pairs :D :D\n",
      "Lenny: yeah, but shouldn't I pick a different color?\n",
      "Bob: what matters is what you'll give you the most outfit options\n",
      "Lenny: So I guess I'll buy the first or the third pair then\n",
      "Bob: Pick the best quality then\n",
      "Lenny: ur right, thx\n",
      "Bob: no prob :)\n",
      "\n",
      "Generated Summary:\n",
      "Lenny: Babe, can you help me with something? Bob: Sure, what's up? Lenny: Which one should I pick?Bob: Send me photos. Lenny asks: Does it make sense to have two pairs of purple trousers. Bob: I have four black\n",
      "\n",
      "Example 4:\n",
      "Dialogue:\n",
      "Will: hey babe, what do you want for dinner tonight?\n",
      "Emma:  gah, don't even worry about it tonight\n",
      "Will: what do you mean? everything ok?\n",
      "Emma: not really, but it's ok, don't worry about cooking though, I'm not hungry\n",
      "Will: Well what time will you be home?\n",
      "Emma: soon, hopefully\n",
      "Will: you sure? Maybe you want me to pick you up?\n",
      "Emma: no no it's alright. I'll be home soon, i'll tell you when I get home. \n",
      "Will: Alright, love you. \n",
      "Emma: love you too. \n",
      "\n",
      "Generated Summary:\n",
      "Will: hey babe, what do you want for dinner tonight? Emma:  gah, don't even worry about it tonight. Will: Maybe you want me to pick you up?Emma: no no it's alright. I'll be home soon, i'll tell you\n",
      "\n",
      "Example 5:\n",
      "Dialogue:\n",
      "Ollie: Hi , are you in Warsaw\n",
      "Jane: yes, just back! Btw are you free for diner the 19th?\n",
      "Ollie: nope!\n",
      "Jane: and the  18th?\n",
      "Ollie: nope, we have this party and you must be there, remember?\n",
      "Jane: oh right! i lost my calendar..  thanks for reminding me\n",
      "Ollie: we have lunch this week?\n",
      "Jane: with pleasure!\n",
      "Ollie: friday?\n",
      "Jane: ok\n",
      "Jane: what do you mean \" we don't have any more whisky!\" lol..\n",
      "Ollie: what!!!\n",
      "Jane: you just call me and the all thing i heard was that sentence about whisky... what's wrong with you?\n",
      "Ollie: oh oh... very strange! i have to be carefull may be there is some spy in my mobile! lol\n",
      "Jane: dont' worry, we'll check on friday.\n",
      "Ollie: don't forget to bring some sun with you\n",
      "Jane: I can't wait to be in Morocco..\n",
      "Ollie: enjoy and see you friday\n",
      "Jane: sorry Ollie, i'm very busy, i won't have time for lunch  tomorrow, but may be at 6pm after my courses?this trip to Morocco was so nice, but time consuming!\n",
      "Ollie: ok for tea!\n",
      "Jane: I'm on my way..\n",
      "Ollie: tea is ready, did you bring the pastries?\n",
      "Jane: I already ate them all... see you in a minute\n",
      "Ollie: ok\n",
      "\n",
      "Generated Summary:\n",
      "Ollie calls his friend in Warsaw to ask if he is free for lunch on the 19th and 18th. He also wants to invite him to a party on the 18th, but he has lost his calendar. Ollie and his friend are on their way to\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load pre-trained summarization model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Example input texts (first 5 test dialogues)\n",
    "example_inputs = test['dialogue'].head(5).tolist()\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(example_inputs, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Generate summaries\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], max_length=60, num_beams=4, early_stopping=True)\n",
    "\n",
    "# Decode summaries\n",
    "summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "# Print input dialogues and generated summaries\n",
    "for i, (dialogue, summary) in enumerate(zip(example_inputs, summaries)):\n",
    "    print(f\"\\nExample {i+1}:\\nDialogue:\\n{dialogue}\\n\\nGenerated Summary:\\n{summary}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
